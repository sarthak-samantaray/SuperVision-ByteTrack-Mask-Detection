{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1FMggEP0CmFwodvj0HBtXbL6AXBbZqRnS","authorship_tag":"ABX9TyPmQUX8Pn4oOYgALqDROVqv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"6a495c17ad85468e8d773fee1bfe0322":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b932c025c2e04a81972b39546b798771","IPY_MODEL_c2206c1d562548869d286a3045fcfbcf","IPY_MODEL_bea9dd407278498ab801c5d20dd6476f"],"layout":"IPY_MODEL_ff6c2f29975745e286c607d451eadf36"}},"b932c025c2e04a81972b39546b798771":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae996443c0244077b1e350c39c6bfecf","placeholder":"â€‹","style":"IPY_MODEL_f2519c2dcee34f30ab857d799e565fed","value":"100%"}},"c2206c1d562548869d286a3045fcfbcf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_269bf23be71f4404b1d5ee3d3fbab2b4","max":538,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8be3137ff0684c31b259838af74f6aed","value":538}},"bea9dd407278498ab801c5d20dd6476f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_88e42c420e4a453481a1d01375bc1b2d","placeholder":"â€‹","style":"IPY_MODEL_2e26d80b0c664de5928fa894743973b1","value":" 538/538 [00:38&lt;00:00, 15.45it/s]"}},"ff6c2f29975745e286c607d451eadf36":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae996443c0244077b1e350c39c6bfecf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2519c2dcee34f30ab857d799e565fed":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"269bf23be71f4404b1d5ee3d3fbab2b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8be3137ff0684c31b259838af74f6aed":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"88e42c420e4a453481a1d01375bc1b2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e26d80b0c664de5928fa894743973b1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"a4s7zCKGOpbL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682876903697,"user_tz":-330,"elapsed":15083,"user":{"displayName":"sarthak samantaray","userId":"02239945013595992252"}},"outputId":"bc9e9d1c-b4e1-4537-96d7-ce8a5886eaf3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os \n","os.chdir(\"/content/drive/MyDrive/AI projects/1. Car Counter\")\n","home = os.getcwd()\n","home"],"metadata":{"id":"4xM46EQ4UBkS","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1682876909663,"user_tz":-330,"elapsed":574,"user":{"displayName":"sarthak samantaray","userId":"02239945013595992252"}},"outputId":"a74b7dfa-b45c-4cd8-a45b-78d260bce55e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/AI projects/1. Car Counter'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["!pip install loguru\n","!pip install lap\n","!pip install ultralytics\n","\n","\n","!git clone https://github.com/ifzhang/ByteTrack.git\n","%cd {home}/ByteTrack\n","!sed -i 's/onnx==1.8.1/onnx==1.9.0/g' requirements.txt\n","\n","!pip3 install -q -r requirements.txt\n","!python3 setup.py -q develop\n","!pip install -q cython_bbox\n","!pip install -q onemetric\n","\n","!pip install supervision==0.1.0\n","\n","!pip install cvzone"],"metadata":{"id":"r6SYXO_rUMsJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682877000289,"user_tz":-330,"elapsed":22,"user":{"displayName":"sarthak samantaray","userId":"02239945013595992252"}},"outputId":"caa9f8bf-0b2f-46c8-ea62-3c47ed3ebe77"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Installing collected packages: cvzone\n","Successfully installed cvzone-1.5.6\n"]}]},{"cell_type":"code","source":["# Checking everything.\n","import ultralytics \n","print(ultralytics.checks())\n","\n","import supervision\n","print(f\"Supervision version = {supervision.__version__}\")\n"," \n","import yolox\n","print(f\"yolox verison = {yolox.__version__}\")"],"metadata":{"id":"o4zL9ZIAUP7R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682877008808,"user_tz":-330,"elapsed":8529,"user":{"displayName":"sarthak samantaray","userId":"02239945013595992252"}},"outputId":"2a0cd532-cc2a-40d6-b06b-27763cf6af16"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Ultralytics YOLOv8.0.90 ðŸš€ Python-3.10.11 torch-2.0.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","Setup complete âœ… (2 CPUs, 12.7 GB RAM, 23.5/78.2 GB disk)\n"]},{"output_type":"stream","name":"stdout","text":["None\n","Supervision version = 0.1.0\n","yolox verison = 0.1.0\n"]}]},{"cell_type":"code","source":["from supervision.video.source import get_video_frames_generator\n","from supervision.draw.color import ColorPalette,Color\n","from supervision.notebook.utils import show_frame_in_notebook\n","from supervision.tools.detections import Detections,BoxAnnotator\n","from supervision.video.sink import VideoSink # To save the video.\n","from supervision.video.dataclasses import VideoInfo\n","from supervision.tools.line_counter import LineCounter, LineCounterAnnotator\n","from supervision.geometry.dataclasses import Point\n","from tqdm.notebook import tqdm\n","import numpy as np\n","from ultralytics import YOLO"],"metadata":{"id":"TP8zfD64UrRi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from yolox.tracker.byte_tracker import BYTETracker, STrack\n","from onemetric.cv.utils.iou import box_iou_batch\n","from dataclasses import dataclass\n","\n","\n","@dataclass(frozen=True)\n","class BYTETrackerArgs:\n","    track_thresh: float = 0.25\n","    track_buffer: int = 30\n","    match_thresh: float = 0.8\n","    aspect_ratio_thresh: float = 3.0\n","    min_box_area: float = 1.0\n","    mot20: bool = False"],"metadata":{"id":"pX_xXgjReo1j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing import List\n","\n","import numpy as np\n","\n","\n","# converts Detections into format that can be consumed by match_detections_with_tracks function\n","def detections2boxes(detections: Detections) -> np.ndarray:\n","    # This will just horizontally stack the two values, looks like this [1,2,3,4,5] , 1 to 4 are the location, 5 is the conf.\n","    return np.hstack((\n","        detections.xyxy,\n","        # It makes confidence of each object in a seperate array. if conf = [1,2,3,4] then will change it into [1],[2],[3]...\n","        detections.confidence[:, np.newaxis]\n","    ))\n","\n","\n","# converts List[STrack] into format that can be consumed by match_detections_with_tracks function\n","# This will turn the tracks in to xmin,ymin,xmax,ymax\n","def tracks2boxes(tracks: List[STrack]) -> np.ndarray:\n","    return np.array([\n","        track.tlbr\n","        for track\n","        in tracks\n","    ], dtype=float)\n","\n","\n","\n","\n","\n","\n","\n","# This function takes in a set of detections and a list of tracks and matches the detections to the \n","# corresponding tracks based on their bounding box coordinates.\n","\n","# First, it checks if there are any detections or tracks. If there are none, it returns an empty array.\n","\n","# Next, it converts the tracks to bounding boxes using the tracks2boxes function and computes \n","# the intersection over union (IoU) between each track's bounding box and each detection's bounding box using the box_iou_batch function.\n","\n","# Then, it finds the index of the detection with the highest IoU for each track using np.argmax, and stores these indices in track2detection.\n","\n","# The function then initializes an empty list called tracker_ids with the same length as the number of detections. For each track, \n","# it checks if the highest IoU between the track and the detections is not zero. If it's not zero, it stores the track's ID in tracker_ids at \n","# the index corresponding to the detection with the highest IoU.\n","\n","# Finally, the function returns the list of tracker IDs for each detection.\n","\n","# matches our bounding boxes with predictions\n","def match_detections_with_tracks(\n","    detections: Detections, \n","    tracks: List[STrack]\n",") -> Detections:\n","    if not np.any(detections.xyxy) or len(tracks) == 0:\n","        return np.empty((0,))\n","\n","    tracks_boxes = tracks2boxes(tracks=tracks)\n","    iou = box_iou_batch(tracks_boxes, detections.xyxy)\n","    track2detection = np.argmax(iou, axis=1)\n","    \n","    tracker_ids = [None] * len(detections)\n","    \n","    for tracker_index, detection_index in enumerate(track2detection):\n","        if iou[tracker_index, detection_index] != 0:\n","            tracker_ids[detection_index] = tracks[tracker_index].track_id\n","\n","    return tracker_ids"],"metadata":{"id":"RejVdnUyesLZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Settings \n","# LINE_START = Point(0,550)\n","# LINE_END = Point(1920,100)\n","\n","TARGET_VIDEO_PATH = f\"{home}/Videos/WIN_20230430_20_05_38_Pro-RESULT101.mp4\""],"metadata":{"id":"JaSxZrXCetog"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# The Flow\n","1. Video Source\n","2. Model\n","3. model.fuse()\n","4. class_id (the ones you want to be detected)\n","5. class_names_dict (the names of the classes)\n","6. initialize byte tracker\n","7. video_info(takes in source path)\n","8. create generator\n","9. create line counter instance\n","10. box_annotator instance\n","11. line annotator instance\n","\n","12. with videoSink(target_video,video_info) as sink:\n","13. loop over frames\n","14. result\n","15. detections\n","16. filtering out detections with unwanted classes\n","\n","17. tracking detections\n","18. extracting tracker id\n","19. filtering out detections without trackers\n","20. Labels\n","21. updating line counter\n","22. make bbox\n","23. make line\n","24. sink.write to save the video."],"metadata":{"id":"oUPSWb4_fNig"}},{"cell_type":"code","source":["import cv2\n","import cvzone\n","\n","\n","\n","# video source\n","video_source_path = f\"{home}/Videos/WIN_20230430_20_05_38_Pro.mp4\"\n","\n","# model\n","model = YOLO(f\"{home}/yolo_weights/best.pt\")\n","model.fuse()\n","\n","# video_info\n","video_info = VideoInfo.from_video_path(video_source_path)\n","\n","# generator\n","generator = get_video_frames_generator(video_source_path)\n","\n","# CLASS ID\n","CLASS_ID = [4,6]\n","\n","# CLASS_NAMES_DICT\n","CLASS_NAMES_DICT = model.model.names\n","\n","with VideoSink(TARGET_VIDEO_PATH , video_info) as sink:\n","  # loop over frames\n","  for frame in tqdm(generator , total = video_info.total_frames):\n","    # results\n","    results = model(frame)\n","    # detections\n","    detections = Detections(\n","        xyxy = results[0].boxes.xyxy.cpu().numpy(),\n","        confidence = results[0].boxes.conf.cpu().numpy(),\n","        class_id = results[0].boxes.cls.cpu().numpy().astype(int)\n","    )\n","\n","    # filtering\n","    mask = np.array([class_id in CLASS_ID for class_id in detections.class_id],dtype = bool)\n","    detections.filter(mask=mask , inplace= True)\n","\n","    \n","    labels = [\n","        f\"{CLASS_NAMES_DICT[class_id]} {confidence:0.5f}\"\n","        for _, confidence , class_id, tracker_id in detections\n","    ]\n","    if labels:\n","      if labels[0].split()[0] == \"Mask\":\n","        box_annotator = BoxAnnotator(color = Color(0,255,0), thickness = 1, text_thickness = 1, text_scale = 0.5)\n","      elif labels[0].split()[0] == \"NO-Mask\":\n","        box_annotator = BoxAnnotator(color = Color(255,0,0), thickness = 1, text_thickness = 1, text_scale = 0.5)\n","    else:\n","      box_annotator = BoxAnnotator(color = Color(255,0,0), thickness = 1, text_thickness = 1, text_scale = 0.5)\n","\n","    box_annotator.annotate(frame,detections = detections , labels = labels)\n","    sink.write_frame(frame)"],"metadata":{"id":"h0SR6Xe4foa5","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["6a495c17ad85468e8d773fee1bfe0322","b932c025c2e04a81972b39546b798771","c2206c1d562548869d286a3045fcfbcf","bea9dd407278498ab801c5d20dd6476f","ff6c2f29975745e286c607d451eadf36","ae996443c0244077b1e350c39c6bfecf","f2519c2dcee34f30ab857d799e565fed","269bf23be71f4404b1d5ee3d3fbab2b4","8be3137ff0684c31b259838af74f6aed","88e42c420e4a453481a1d01375bc1b2d","2e26d80b0c664de5928fa894743973b1"]},"executionInfo":{"status":"ok","timestamp":1682880759547,"user_tz":-330,"elapsed":40550,"user":{"displayName":"sarthak samantaray","userId":"02239945013595992252"}},"outputId":"7ad5e4cd-1d2b-4534-9c0c-0c411208dccd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Model summary (fused): 268 layers, 43625883 parameters, 0 gradients, 164.9 GFLOPs\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/538 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a495c17ad85468e8d773fee1bfe0322"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 42.2ms\n","Speed: 2.6ms preprocess, 42.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 41.0ms\n","Speed: 3.7ms preprocess, 41.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 28.4ms\n","Speed: 3.4ms preprocess, 28.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 28.6ms\n","Speed: 3.0ms preprocess, 28.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 28.8ms\n","Speed: 3.1ms preprocess, 28.8ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 27.5ms\n","Speed: 3.0ms preprocess, 27.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 27.3ms\n","Speed: 3.3ms preprocess, 27.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 26.5ms\n","Speed: 3.1ms preprocess, 26.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 26.7ms\n","Speed: 3.5ms preprocess, 26.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 26.7ms\n","Speed: 3.7ms preprocess, 26.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.9ms\n","Speed: 3.1ms preprocess, 25.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.9ms\n","Speed: 3.7ms preprocess, 25.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 26.0ms\n","Speed: 3.6ms preprocess, 26.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.9ms\n","Speed: 3.9ms preprocess, 25.9ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.9ms\n","Speed: 3.7ms preprocess, 25.9ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 26.0ms\n","Speed: 3.3ms preprocess, 26.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.9ms\n","Speed: 2.8ms preprocess, 25.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.9ms\n","Speed: 3.2ms preprocess, 25.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 26.3ms\n","Speed: 3.5ms preprocess, 26.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 3.0ms preprocess, 25.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 26.9ms\n","Speed: 4.9ms preprocess, 26.9ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.3ms\n","Speed: 5.6ms preprocess, 25.3ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 34.0ms\n","Speed: 5.4ms preprocess, 34.0ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 32.2ms\n","Speed: 5.2ms preprocess, 32.2ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 27.8ms\n","Speed: 10.7ms preprocess, 27.8ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 29.8ms\n","Speed: 3.5ms preprocess, 29.8ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.4ms\n","Speed: 3.5ms preprocess, 24.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.4ms\n","Speed: 3.2ms preprocess, 24.4ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.4ms\n","Speed: 3.4ms preprocess, 24.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.4ms\n","Speed: 8.6ms preprocess, 24.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.4ms\n","Speed: 2.9ms preprocess, 24.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.4ms\n","Speed: 4.8ms preprocess, 24.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.4ms\n","Speed: 3.2ms preprocess, 24.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 6.3ms preprocess, 25.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.5ms\n","Speed: 3.9ms preprocess, 24.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.4ms\n","Speed: 6.4ms preprocess, 24.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.2ms\n","Speed: 4.0ms preprocess, 24.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.0ms\n","Speed: 2.9ms preprocess, 24.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 23.4ms\n","Speed: 2.7ms preprocess, 23.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 23.4ms\n","Speed: 3.0ms preprocess, 23.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.7ms\n","Speed: 3.5ms preprocess, 22.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 23.6ms\n","Speed: 3.2ms preprocess, 23.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.3ms\n","Speed: 3.2ms preprocess, 22.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.3ms\n","Speed: 3.4ms preprocess, 22.3ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.2ms\n","Speed: 3.3ms preprocess, 22.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.2ms\n","Speed: 5.5ms preprocess, 22.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.2ms\n","Speed: 3.0ms preprocess, 22.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.2ms\n","Speed: 5.1ms preprocess, 22.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.4ms\n","Speed: 6.7ms preprocess, 22.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.2ms\n","Speed: 3.3ms preprocess, 22.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.2ms\n","Speed: 3.4ms preprocess, 22.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.2ms\n","Speed: 3.9ms preprocess, 22.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.7ms\n","Speed: 3.4ms preprocess, 22.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.7ms\n","Speed: 3.6ms preprocess, 22.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.3ms\n","Speed: 3.0ms preprocess, 22.3ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.2ms\n","Speed: 3.8ms preprocess, 22.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 20.6ms\n","Speed: 3.0ms preprocess, 20.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 20.5ms\n","Speed: 4.7ms preprocess, 20.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 20.4ms\n","Speed: 3.2ms preprocess, 20.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 20.6ms\n","Speed: 2.7ms preprocess, 20.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 20.6ms\n","Speed: 3.2ms preprocess, 20.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 20.4ms\n","Speed: 3.9ms preprocess, 20.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 20.6ms\n","Speed: 3.2ms preprocess, 20.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.0ms\n","Speed: 3.8ms preprocess, 21.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 20.7ms\n","Speed: 3.4ms preprocess, 20.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 20.8ms\n","Speed: 3.2ms preprocess, 20.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 20.6ms\n","Speed: 3.1ms preprocess, 20.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 20.7ms\n","Speed: 3.0ms preprocess, 20.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 20.8ms\n","Speed: 4.4ms preprocess, 20.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 20.6ms\n","Speed: 2.9ms preprocess, 20.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 20.6ms\n","Speed: 3.2ms preprocess, 20.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 20.5ms\n","Speed: 3.2ms preprocess, 20.5ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 20.4ms\n","Speed: 3.5ms preprocess, 20.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 20.6ms\n","Speed: 3.1ms preprocess, 20.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 20.4ms\n","Speed: 3.6ms preprocess, 20.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.5ms\n","Speed: 4.2ms preprocess, 21.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 20.4ms\n","Speed: 4.1ms preprocess, 20.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 20.7ms\n","Speed: 3.8ms preprocess, 20.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 20.5ms\n","Speed: 3.1ms preprocess, 20.5ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 20.5ms\n","Speed: 3.4ms preprocess, 20.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 20.4ms\n","Speed: 3.0ms preprocess, 20.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 20.5ms\n","Speed: 3.2ms preprocess, 20.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 20.4ms\n","Speed: 3.0ms preprocess, 20.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.0ms\n","Speed: 3.5ms preprocess, 22.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 20.4ms\n","Speed: 3.2ms preprocess, 20.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 20.4ms\n","Speed: 5.2ms preprocess, 20.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.0ms\n","Speed: 3.5ms preprocess, 21.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.0ms\n","Speed: 2.8ms preprocess, 21.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 20.8ms\n","Speed: 3.4ms preprocess, 20.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.4ms\n","Speed: 3.2ms preprocess, 22.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.5ms\n","Speed: 3.2ms preprocess, 21.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.2ms\n","Speed: 4.6ms preprocess, 21.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 30.3ms\n","Speed: 3.9ms preprocess, 30.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.9ms\n","Speed: 5.3ms preprocess, 21.9ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.1ms\n","Speed: 3.6ms preprocess, 21.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.6ms\n","Speed: 6.5ms preprocess, 21.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.9ms\n","Speed: 4.4ms preprocess, 21.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.4ms\n","Speed: 3.5ms preprocess, 21.4ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.6ms\n","Speed: 3.8ms preprocess, 21.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.3ms\n","Speed: 4.2ms preprocess, 22.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.5ms\n","Speed: 3.0ms preprocess, 21.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.6ms\n","Speed: 3.0ms preprocess, 21.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.6ms\n","Speed: 3.5ms preprocess, 21.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.6ms\n","Speed: 2.9ms preprocess, 21.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.3ms\n","Speed: 3.3ms preprocess, 22.3ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.5ms\n","Speed: 3.5ms preprocess, 22.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.3ms\n","Speed: 3.2ms preprocess, 22.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.7ms\n","Speed: 3.1ms preprocess, 21.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.1ms\n","Speed: 3.2ms preprocess, 22.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.9ms\n","Speed: 4.2ms preprocess, 22.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.2ms\n","Speed: 5.0ms preprocess, 22.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.5ms\n","Speed: 3.3ms preprocess, 22.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.4ms\n","Speed: 4.7ms preprocess, 22.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.0ms\n","Speed: 3.2ms preprocess, 22.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.8ms\n","Speed: 3.0ms preprocess, 21.8ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.7ms\n","Speed: 4.5ms preprocess, 22.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.7ms\n","Speed: 3.2ms preprocess, 22.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.5ms\n","Speed: 4.4ms preprocess, 22.5ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 23.0ms\n","Speed: 3.7ms preprocess, 23.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 23.1ms\n","Speed: 3.2ms preprocess, 23.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.5ms\n","Speed: 3.6ms preprocess, 22.5ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.6ms\n","Speed: 2.7ms preprocess, 22.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.0ms\n","Speed: 3.0ms preprocess, 22.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 23.3ms\n","Speed: 3.0ms preprocess, 23.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.0ms\n","Speed: 3.1ms preprocess, 22.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.0ms\n","Speed: 3.1ms preprocess, 22.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.0ms\n","Speed: 3.0ms preprocess, 22.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.0ms\n","Speed: 3.2ms preprocess, 22.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.8ms\n","Speed: 3.1ms preprocess, 21.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.4ms\n","Speed: 3.2ms preprocess, 21.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.4ms\n","Speed: 3.1ms preprocess, 21.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.6ms\n","Speed: 3.7ms preprocess, 21.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.5ms\n","Speed: 3.1ms preprocess, 21.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.4ms\n","Speed: 6.1ms preprocess, 21.4ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.4ms\n","Speed: 3.1ms preprocess, 21.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.5ms\n","Speed: 5.1ms preprocess, 21.5ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.9ms\n","Speed: 3.3ms preprocess, 21.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.4ms\n","Speed: 4.4ms preprocess, 22.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.9ms\n","Speed: 3.8ms preprocess, 21.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.1ms\n","Speed: 3.2ms preprocess, 22.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.4ms\n","Speed: 3.4ms preprocess, 21.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.9ms\n","Speed: 3.1ms preprocess, 21.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.1ms\n","Speed: 3.3ms preprocess, 22.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.1ms\n","Speed: 3.8ms preprocess, 22.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.0ms\n","Speed: 3.2ms preprocess, 22.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.9ms\n","Speed: 3.0ms preprocess, 21.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.9ms\n","Speed: 3.2ms preprocess, 21.9ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.6ms\n","Speed: 3.3ms preprocess, 22.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.1ms\n","Speed: 3.2ms preprocess, 22.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.0ms\n","Speed: 4.6ms preprocess, 22.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.9ms\n","Speed: 3.1ms preprocess, 21.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 2 NO-Safety Vests, 1 Person, 25.2ms\n","Speed: 3.2ms preprocess, 25.2ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 2 NO-Safety Vests, 1 Person, 22.2ms\n","Speed: 10.2ms preprocess, 22.2ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 23.5ms\n","Speed: 3.1ms preprocess, 23.5ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.3ms\n","Speed: 2.9ms preprocess, 25.3ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 Person, 22.9ms\n","Speed: 5.0ms preprocess, 22.9ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 26.2ms\n","Speed: 3.5ms preprocess, 26.2ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.7ms\n","Speed: 4.9ms preprocess, 25.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 Person, 24.5ms\n","Speed: 3.2ms preprocess, 24.5ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 24.5ms\n","Speed: 3.0ms preprocess, 24.5ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 24.0ms\n","Speed: 2.9ms preprocess, 24.0ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.7ms\n","Speed: 3.1ms preprocess, 25.7ms inference, 6.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 24.0ms\n","Speed: 3.2ms preprocess, 24.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 Person, 30.1ms\n","Speed: 3.0ms preprocess, 30.1ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.5ms\n","Speed: 3.1ms preprocess, 25.5ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 26.8ms\n","Speed: 7.1ms preprocess, 26.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 24.9ms\n","Speed: 3.0ms preprocess, 24.9ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.0ms\n","Speed: 10.0ms preprocess, 25.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 26.0ms\n","Speed: 3.1ms preprocess, 26.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 26.1ms\n","Speed: 3.3ms preprocess, 26.1ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 5.8ms preprocess, 25.4ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 32.3ms\n","Speed: 3.3ms preprocess, 32.3ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 26.5ms\n","Speed: 3.1ms preprocess, 26.5ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 27.3ms\n","Speed: 6.4ms preprocess, 27.3ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 27.8ms\n","Speed: 3.0ms preprocess, 27.8ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 30.5ms\n","Speed: 3.3ms preprocess, 30.5ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 27.1ms\n","Speed: 3.1ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 29.1ms\n","Speed: 6.2ms preprocess, 29.1ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 28.5ms\n","Speed: 3.2ms preprocess, 28.5ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 27.3ms\n","Speed: 3.0ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 28.5ms\n","Speed: 6.8ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 27.8ms\n","Speed: 2.9ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 29.4ms\n","Speed: 9.4ms preprocess, 29.4ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 28.4ms\n","Speed: 2.8ms preprocess, 28.4ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 28.0ms\n","Speed: 2.9ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 27.7ms\n","Speed: 3.2ms preprocess, 27.7ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 32.7ms\n","Speed: 5.0ms preprocess, 32.7ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 28.8ms\n","Speed: 2.9ms preprocess, 28.8ms inference, 5.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 28.5ms\n","Speed: 3.5ms preprocess, 28.5ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 Person, 28.6ms\n","Speed: 6.1ms preprocess, 28.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 34.9ms\n","Speed: 3.0ms preprocess, 34.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 28.5ms\n","Speed: 3.4ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 28.8ms\n","Speed: 5.6ms preprocess, 28.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 Person, 28.9ms\n","Speed: 3.2ms preprocess, 28.9ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 Person, 34.9ms\n","Speed: 3.9ms preprocess, 34.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 Person, 30.2ms\n","Speed: 2.9ms preprocess, 30.2ms inference, 4.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 30.8ms\n","Speed: 3.1ms preprocess, 30.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 30.0ms\n","Speed: 5.2ms preprocess, 30.0ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 Person, 35.7ms\n","Speed: 3.1ms preprocess, 35.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 Person, 32.9ms\n","Speed: 2.9ms preprocess, 32.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 Person, 30.5ms\n","Speed: 5.0ms preprocess, 30.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 Person, 29.6ms\n","Speed: 5.0ms preprocess, 29.6ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 Person, 30.7ms\n","Speed: 3.1ms preprocess, 30.7ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 29.6ms\n","Speed: 3.0ms preprocess, 29.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 30.4ms\n","Speed: 2.8ms preprocess, 30.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 29.6ms\n","Speed: 2.9ms preprocess, 29.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 29.6ms\n","Speed: 6.4ms preprocess, 29.6ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 29.7ms\n","Speed: 3.0ms preprocess, 29.7ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 31.0ms\n","Speed: 3.0ms preprocess, 31.0ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 36.5ms\n","Speed: 4.6ms preprocess, 36.5ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 29.6ms\n","Speed: 4.7ms preprocess, 29.6ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 30.8ms\n","Speed: 4.6ms preprocess, 30.8ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 33.2ms\n","Speed: 3.0ms preprocess, 33.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 29.8ms\n","Speed: 5.4ms preprocess, 29.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 29.6ms\n","Speed: 3.0ms preprocess, 29.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 29.8ms\n","Speed: 3.4ms preprocess, 29.8ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 30.4ms\n","Speed: 3.2ms preprocess, 30.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 32.3ms\n","Speed: 3.0ms preprocess, 32.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 29.2ms\n","Speed: 3.8ms preprocess, 29.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 29.6ms\n","Speed: 3.1ms preprocess, 29.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 28.2ms\n","Speed: 3.2ms preprocess, 28.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 27.6ms\n","Speed: 3.3ms preprocess, 27.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 26.5ms\n","Speed: 3.0ms preprocess, 26.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 26.6ms\n","Speed: 3.8ms preprocess, 26.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 26.6ms\n","Speed: 3.9ms preprocess, 26.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 26.7ms\n","Speed: 6.6ms preprocess, 26.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 26.3ms\n","Speed: 3.1ms preprocess, 26.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 26.1ms\n","Speed: 3.2ms preprocess, 26.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 3.0ms preprocess, 25.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 35.1ms\n","Speed: 8.9ms preprocess, 35.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 51.8ms\n","Speed: 10.9ms preprocess, 51.8ms inference, 12.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 3.5ms preprocess, 25.4ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 3.6ms preprocess, 25.4ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.5ms\n","Speed: 5.3ms preprocess, 25.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 3.3ms preprocess, 25.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.6ms\n","Speed: 5.3ms preprocess, 25.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 3.3ms preprocess, 25.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.9ms\n","Speed: 3.3ms preprocess, 25.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.7ms\n","Speed: 3.5ms preprocess, 25.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 4.4ms preprocess, 25.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 46.4ms\n","Speed: 8.2ms preprocess, 46.4ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 32.6ms\n","Speed: 3.1ms preprocess, 32.6ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 5.9ms preprocess, 25.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 26.6ms\n","Speed: 4.1ms preprocess, 26.6ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 3.3ms preprocess, 25.4ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 3.3ms preprocess, 25.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 28.5ms\n","Speed: 3.8ms preprocess, 28.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.6ms\n","Speed: 8.5ms preprocess, 25.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.0ms\n","Speed: 3.0ms preprocess, 25.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.0ms\n","Speed: 3.0ms preprocess, 25.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.0ms\n","Speed: 3.3ms preprocess, 25.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 26.3ms\n","Speed: 3.3ms preprocess, 26.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.0ms\n","Speed: 3.5ms preprocess, 25.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.0ms\n","Speed: 3.2ms preprocess, 25.0ms inference, 4.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.0ms\n","Speed: 3.3ms preprocess, 25.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.0ms\n","Speed: 3.4ms preprocess, 25.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 26.0ms\n","Speed: 4.0ms preprocess, 26.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 3.1ms preprocess, 25.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.3ms\n","Speed: 3.0ms preprocess, 25.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 3.3ms preprocess, 25.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 30.9ms\n","Speed: 2.9ms preprocess, 30.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 3.1ms preprocess, 25.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 3.0ms preprocess, 25.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 3.1ms preprocess, 25.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 3.1ms preprocess, 25.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 5.5ms preprocess, 25.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 3.9ms preprocess, 25.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 3.2ms preprocess, 25.4ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 3.6ms preprocess, 25.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 8.9ms preprocess, 25.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 3.2ms preprocess, 25.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 3.5ms preprocess, 25.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 2.9ms preprocess, 25.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.5ms\n","Speed: 3.2ms preprocess, 25.5ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 3.1ms preprocess, 25.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.8ms\n","Speed: 3.1ms preprocess, 25.8ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.6ms\n","Speed: 3.1ms preprocess, 25.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.5ms\n","Speed: 3.1ms preprocess, 25.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.6ms\n","Speed: 3.2ms preprocess, 25.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.7ms\n","Speed: 3.5ms preprocess, 25.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.3ms\n","Speed: 3.3ms preprocess, 25.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 3.4ms preprocess, 25.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 3.1ms preprocess, 25.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.3ms\n","Speed: 3.0ms preprocess, 25.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.3ms\n","Speed: 3.3ms preprocess, 25.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 3.2ms preprocess, 25.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 3.0ms preprocess, 25.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 Person, 25.4ms\n","Speed: 5.0ms preprocess, 25.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 Person, 31.1ms\n","Speed: 3.3ms preprocess, 31.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 Person, 25.4ms\n","Speed: 3.5ms preprocess, 25.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 Person, 25.4ms\n","Speed: 3.6ms preprocess, 25.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.7ms\n","Speed: 9.0ms preprocess, 25.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 2 NO-Safety Vests, 1 Person, 26.6ms\n","Speed: 3.1ms preprocess, 26.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 26.6ms\n","Speed: 3.6ms preprocess, 26.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.6ms\n","Speed: 3.4ms preprocess, 25.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.3ms\n","Speed: 3.3ms preprocess, 25.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.2ms\n","Speed: 3.2ms preprocess, 25.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.4ms\n","Speed: 3.0ms preprocess, 24.4ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.5ms\n","Speed: 5.2ms preprocess, 24.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.5ms\n","Speed: 3.3ms preprocess, 24.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.4ms\n","Speed: 5.4ms preprocess, 24.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.5ms\n","Speed: 3.4ms preprocess, 24.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.4ms\n","Speed: 3.0ms preprocess, 24.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 26.6ms\n","Speed: 3.3ms preprocess, 26.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.4ms\n","Speed: 3.1ms preprocess, 24.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 29.2ms\n","Speed: 3.1ms preprocess, 29.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.4ms\n","Speed: 5.0ms preprocess, 24.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 28.3ms\n","Speed: 3.1ms preprocess, 28.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.7ms\n","Speed: 3.1ms preprocess, 24.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.4ms\n","Speed: 11.3ms preprocess, 24.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.4ms\n","Speed: 3.2ms preprocess, 24.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 27.9ms\n","Speed: 6.3ms preprocess, 27.9ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.4ms\n","Speed: 3.5ms preprocess, 24.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.7ms\n","Speed: 11.7ms preprocess, 24.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.4ms\n","Speed: 5.2ms preprocess, 24.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.4ms\n","Speed: 3.3ms preprocess, 24.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.4ms\n","Speed: 3.2ms preprocess, 24.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.6ms\n","Speed: 3.4ms preprocess, 24.6ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.4ms\n","Speed: 3.0ms preprocess, 24.4ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.2ms\n","Speed: 3.2ms preprocess, 24.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.7ms\n","Speed: 3.1ms preprocess, 22.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.3ms\n","Speed: 3.1ms preprocess, 22.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.3ms\n","Speed: 3.0ms preprocess, 24.3ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.2ms\n","Speed: 3.2ms preprocess, 22.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.2ms\n","Speed: 3.1ms preprocess, 22.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.2ms\n","Speed: 3.2ms preprocess, 22.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.2ms\n","Speed: 3.3ms preprocess, 22.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.5ms\n","Speed: 4.1ms preprocess, 22.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.7ms\n","Speed: 3.3ms preprocess, 22.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.1ms\n","Speed: 3.1ms preprocess, 24.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.3ms\n","Speed: 3.1ms preprocess, 24.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.2ms\n","Speed: 3.1ms preprocess, 22.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.2ms\n","Speed: 2.9ms preprocess, 22.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.7ms\n","Speed: 3.1ms preprocess, 22.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.7ms\n","Speed: 3.1ms preprocess, 22.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.2ms\n","Speed: 3.0ms preprocess, 22.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.3ms\n","Speed: 4.6ms preprocess, 22.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.4ms\n","Speed: 3.0ms preprocess, 22.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.8ms\n","Speed: 3.2ms preprocess, 21.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.8ms\n","Speed: 4.9ms preprocess, 21.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.3ms\n","Speed: 3.2ms preprocess, 22.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.3ms\n","Speed: 4.7ms preprocess, 24.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.0ms\n","Speed: 3.5ms preprocess, 22.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.3ms\n","Speed: 3.5ms preprocess, 22.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.3ms\n","Speed: 3.1ms preprocess, 22.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.8ms\n","Speed: 4.9ms preprocess, 21.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.3ms\n","Speed: 3.0ms preprocess, 22.3ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.9ms\n","Speed: 8.1ms preprocess, 21.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.9ms\n","Speed: 3.1ms preprocess, 21.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 2 NO-Safety Vests, 1 Person, 27.3ms\n","Speed: 3.2ms preprocess, 27.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.0ms\n","Speed: 3.1ms preprocess, 21.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.0ms\n","Speed: 3.0ms preprocess, 21.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.0ms\n","Speed: 2.9ms preprocess, 21.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 22.4ms\n","Speed: 8.2ms preprocess, 22.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 21.0ms\n","Speed: 2.9ms preprocess, 21.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Mask, 2 NO-Safety Vests, 1 Person, 25.0ms\n","Speed: 5.7ms preprocess, 25.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Mask, 1 Person, 20.7ms\n","Speed: 3.1ms preprocess, 20.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 Person, 21.0ms\n","Speed: 3.2ms preprocess, 21.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 Person, 21.0ms\n","Speed: 3.0ms preprocess, 21.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 Person, 25.1ms\n","Speed: 5.0ms preprocess, 25.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 20.7ms\n","Speed: 3.2ms preprocess, 20.7ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 23.2ms\n","Speed: 2.9ms preprocess, 23.2ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 21.2ms\n","Speed: 3.0ms preprocess, 21.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 Person, 20.7ms\n","Speed: 3.1ms preprocess, 20.7ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 22.2ms\n","Speed: 6.8ms preprocess, 22.2ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 20.7ms\n","Speed: 10.2ms preprocess, 20.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 20.7ms\n","Speed: 3.3ms preprocess, 20.7ms inference, 8.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 21.0ms\n","Speed: 3.3ms preprocess, 21.0ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 20.9ms\n","Speed: 6.3ms preprocess, 20.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 31.4ms\n","Speed: 3.0ms preprocess, 31.4ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 27.2ms\n","Speed: 3.1ms preprocess, 27.2ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 20.8ms\n","Speed: 9.6ms preprocess, 20.8ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.6ms\n","Speed: 3.3ms preprocess, 25.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 26.2ms\n","Speed: 3.1ms preprocess, 26.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.8ms\n","Speed: 3.7ms preprocess, 25.8ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.0ms\n","Speed: 7.4ms preprocess, 25.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 30.4ms\n","Speed: 2.8ms preprocess, 30.4ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.5ms\n","Speed: 3.9ms preprocess, 25.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 32.7ms\n","Speed: 3.0ms preprocess, 32.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 27.5ms\n","Speed: 2.8ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 29.8ms\n","Speed: 2.8ms preprocess, 29.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 27.3ms\n","Speed: 2.9ms preprocess, 27.3ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 28.1ms\n","Speed: 3.1ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 33.8ms\n","Speed: 3.3ms preprocess, 33.8ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 27.5ms\n","Speed: 3.3ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 27.9ms\n","Speed: 3.1ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 29.5ms\n","Speed: 2.8ms preprocess, 29.5ms inference, 4.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 27.3ms\n","Speed: 3.3ms preprocess, 27.3ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 27.4ms\n","Speed: 6.5ms preprocess, 27.4ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 26.2ms\n","Speed: 3.1ms preprocess, 26.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 30.8ms\n","Speed: 3.1ms preprocess, 30.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 26.3ms\n","Speed: 8.2ms preprocess, 26.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 27.0ms\n","Speed: 3.1ms preprocess, 27.0ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 27.5ms\n","Speed: 2.9ms preprocess, 27.5ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 26.2ms\n","Speed: 3.0ms preprocess, 26.2ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 26.4ms\n","Speed: 4.9ms preprocess, 26.4ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 26.2ms\n","Speed: 3.9ms preprocess, 26.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 26.2ms\n","Speed: 3.1ms preprocess, 26.2ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 26.5ms\n","Speed: 3.5ms preprocess, 26.5ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 27.9ms\n","Speed: 3.2ms preprocess, 27.9ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 31.0ms\n","Speed: 3.3ms preprocess, 31.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 28.3ms\n","Speed: 3.1ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 32.6ms\n","Speed: 3.1ms preprocess, 32.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 26.2ms\n","Speed: 7.9ms preprocess, 26.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 26.2ms\n","Speed: 3.0ms preprocess, 26.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 28.2ms\n","Speed: 2.9ms preprocess, 28.2ms inference, 5.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 29.6ms\n","Speed: 4.4ms preprocess, 29.6ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 28.8ms\n","Speed: 3.3ms preprocess, 28.8ms inference, 5.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 28.8ms\n","Speed: 3.0ms preprocess, 28.8ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 31.3ms\n","Speed: 3.0ms preprocess, 31.3ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 29.2ms\n","Speed: 5.9ms preprocess, 29.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 33.4ms\n","Speed: 3.0ms preprocess, 33.4ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 40.3ms\n","Speed: 4.8ms preprocess, 40.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 30.1ms\n","Speed: 3.3ms preprocess, 30.1ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 30.9ms\n","Speed: 5.2ms preprocess, 30.9ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 30.5ms\n","Speed: 3.1ms preprocess, 30.5ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 29.6ms\n","Speed: 7.7ms preprocess, 29.6ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 40.2ms\n","Speed: 3.2ms preprocess, 40.2ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 31.2ms\n","Speed: 5.2ms preprocess, 31.2ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 29.7ms\n","Speed: 3.4ms preprocess, 29.7ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 29.7ms\n","Speed: 2.9ms preprocess, 29.7ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 29.7ms\n","Speed: 4.3ms preprocess, 29.7ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 29.8ms\n","Speed: 3.6ms preprocess, 29.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 29.8ms\n","Speed: 5.4ms preprocess, 29.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 Person, 29.6ms\n","Speed: 3.1ms preprocess, 29.6ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 29.8ms\n","Speed: 10.0ms preprocess, 29.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 29.8ms\n","Speed: 3.0ms preprocess, 29.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 28.2ms\n","Speed: 6.1ms preprocess, 28.2ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 28.0ms\n","Speed: 3.7ms preprocess, 28.0ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 28.1ms\n","Speed: 3.5ms preprocess, 28.1ms inference, 3.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 27.3ms\n","Speed: 10.4ms preprocess, 27.3ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 28.5ms\n","Speed: 2.6ms preprocess, 28.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 26.5ms\n","Speed: 3.4ms preprocess, 26.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 26.2ms\n","Speed: 4.9ms preprocess, 26.2ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 28.4ms\n","Speed: 3.0ms preprocess, 28.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 30.1ms\n","Speed: 4.4ms preprocess, 30.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 26.2ms\n","Speed: 3.4ms preprocess, 26.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 26.2ms\n","Speed: 3.2ms preprocess, 26.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 26.2ms\n","Speed: 3.0ms preprocess, 26.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 26.6ms\n","Speed: 3.6ms preprocess, 26.6ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.6ms\n","Speed: 3.3ms preprocess, 25.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 26.0ms\n","Speed: 3.3ms preprocess, 26.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.6ms\n","Speed: 3.0ms preprocess, 25.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.6ms\n","Speed: 3.3ms preprocess, 25.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.6ms\n","Speed: 3.1ms preprocess, 25.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.8ms\n","Speed: 5.1ms preprocess, 25.8ms inference, 4.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.6ms\n","Speed: 3.4ms preprocess, 25.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.7ms\n","Speed: 3.0ms preprocess, 25.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.8ms\n","Speed: 3.2ms preprocess, 25.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.6ms\n","Speed: 3.4ms preprocess, 25.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.6ms\n","Speed: 3.0ms preprocess, 25.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.7ms\n","Speed: 3.2ms preprocess, 25.7ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 29.4ms\n","Speed: 3.0ms preprocess, 29.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 28.3ms\n","Speed: 3.0ms preprocess, 28.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 26.7ms\n","Speed: 3.4ms preprocess, 26.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.8ms\n","Speed: 3.7ms preprocess, 25.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.6ms\n","Speed: 5.6ms preprocess, 25.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.6ms\n","Speed: 3.3ms preprocess, 25.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 29.4ms\n","Speed: 3.2ms preprocess, 29.4ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 26.2ms\n","Speed: 4.1ms preprocess, 26.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.6ms\n","Speed: 7.0ms preprocess, 25.6ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 26.0ms\n","Speed: 3.0ms preprocess, 26.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.6ms\n","Speed: 3.0ms preprocess, 25.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 25.6ms\n","Speed: 4.6ms preprocess, 25.6ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 Person, 25.7ms\n","Speed: 2.7ms preprocess, 25.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 Person, 27.4ms\n","Speed: 6.3ms preprocess, 27.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 Person, 27.8ms\n","Speed: 3.4ms preprocess, 27.8ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 Mask, 1 NO-Hardhat, 1 NO-Safety Vest, 1 Person, 27.7ms\n","Speed: 6.9ms preprocess, 27.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 26.1ms\n","Speed: 3.1ms preprocess, 26.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 26.2ms\n","Speed: 3.3ms preprocess, 26.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.9ms\n","Speed: 3.1ms preprocess, 25.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.9ms\n","Speed: 4.0ms preprocess, 25.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.9ms\n","Speed: 3.0ms preprocess, 25.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.9ms\n","Speed: 3.0ms preprocess, 25.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.9ms\n","Speed: 3.0ms preprocess, 25.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.9ms\n","Speed: 3.4ms preprocess, 25.9ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.9ms\n","Speed: 2.9ms preprocess, 25.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 26.1ms\n","Speed: 4.9ms preprocess, 26.1ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.9ms\n","Speed: 3.2ms preprocess, 25.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 26.1ms\n","Speed: 3.1ms preprocess, 26.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.9ms\n","Speed: 3.3ms preprocess, 25.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 26.0ms\n","Speed: 5.3ms preprocess, 26.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 27.0ms\n","Speed: 3.2ms preprocess, 27.0ms inference, 6.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 27.3ms\n","Speed: 6.9ms preprocess, 27.3ms inference, 5.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 28.3ms\n","Speed: 3.4ms preprocess, 28.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 27.4ms\n","Speed: 3.4ms preprocess, 27.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 27.3ms\n","Speed: 3.2ms preprocess, 27.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 26.6ms\n","Speed: 11.9ms preprocess, 26.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 26.9ms\n","Speed: 3.4ms preprocess, 26.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 29.7ms\n","Speed: 3.3ms preprocess, 29.7ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 26.4ms\n","Speed: 3.1ms preprocess, 26.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 26.1ms\n","Speed: 6.3ms preprocess, 26.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.9ms\n","Speed: 4.4ms preprocess, 25.9ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.9ms\n","Speed: 5.2ms preprocess, 25.9ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 31.0ms\n","Speed: 3.2ms preprocess, 31.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 26.1ms\n","Speed: 3.5ms preprocess, 26.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 26.1ms\n","Speed: 3.0ms preprocess, 26.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 26.0ms\n","Speed: 3.0ms preprocess, 26.0ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.9ms\n","Speed: 2.9ms preprocess, 25.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 27.9ms\n","Speed: 4.7ms preprocess, 27.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.9ms\n","Speed: 2.8ms preprocess, 25.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.9ms\n","Speed: 3.2ms preprocess, 25.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.9ms\n","Speed: 2.8ms preprocess, 25.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 26.1ms\n","Speed: 3.3ms preprocess, 26.1ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 26.0ms\n","Speed: 2.9ms preprocess, 26.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 29.2ms\n","Speed: 2.9ms preprocess, 29.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.9ms\n","Speed: 3.1ms preprocess, 25.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.9ms\n","Speed: 3.0ms preprocess, 25.9ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 26.0ms\n","Speed: 3.6ms preprocess, 26.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 26.1ms\n","Speed: 3.2ms preprocess, 26.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.9ms\n","Speed: 3.3ms preprocess, 25.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 27.3ms\n","Speed: 3.4ms preprocess, 27.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 26.1ms\n","Speed: 3.3ms preprocess, 26.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 26.0ms\n","Speed: 3.3ms preprocess, 26.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.9ms\n","Speed: 3.3ms preprocess, 25.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 26.2ms\n","Speed: 3.4ms preprocess, 26.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.5ms\n","Speed: 5.6ms preprocess, 25.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 4.3ms preprocess, 25.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 3.0ms preprocess, 25.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 3.3ms preprocess, 25.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.5ms\n","Speed: 3.0ms preprocess, 25.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.6ms\n","Speed: 3.3ms preprocess, 25.6ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 2.9ms preprocess, 25.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 3.4ms preprocess, 25.4ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.4ms\n","Speed: 3.4ms preprocess, 25.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.3ms\n","Speed: 3.1ms preprocess, 25.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 26.5ms\n","Speed: 2.8ms preprocess, 26.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.5ms\n","Speed: 3.2ms preprocess, 25.5ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.8ms\n","Speed: 3.0ms preprocess, 24.8ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.9ms\n","Speed: 3.0ms preprocess, 24.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.1ms\n","Speed: 2.9ms preprocess, 25.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 25.3ms\n","Speed: 3.2ms preprocess, 25.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.4ms\n","Speed: 3.2ms preprocess, 24.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.4ms\n","Speed: 3.1ms preprocess, 24.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.4ms\n","Speed: 3.3ms preprocess, 24.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.5ms\n","Speed: 4.8ms preprocess, 24.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.5ms\n","Speed: 3.3ms preprocess, 24.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 NO-Hardhat, 1 NO-Mask, 1 NO-Safety Vest, 1 Person, 24.4ms\n","Speed: 2.9ms preprocess, 24.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"DL8YZWzRhYUi"},"execution_count":null,"outputs":[]}]}